{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multi Layer Perceptron function approximation</h2>\n",
    "\n",
    "In the following example, we implement an MLP network that will be able to learn the trend of an arbitrary function f(x). In the training phase the network will have to learn from a known set of points, that is x and f(x), while in the test phase the network will deduct the values of f(x) only from the x values.\n",
    "\n",
    "This very simple network will be built by a single hidden layer. \n",
    "\n",
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the data model. The function to be learned will follow the trend of the cosine function, evaluated for 1000 points to which we add a very little random error (noise) to reproduce a real case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_points = 1000\n",
    "np.random.seed(NUM_points)\n",
    "function_to_learn = lambda x: np.cos(x) + 0.1*np.random.randn(*x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP network will be formed by a hidden layer of 10 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_neurons = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network learns for 100 points at a time to a total of 1500 learning cycles (epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "NUM_EPOCHS = 1500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_x contiene tutti i punti\n",
    "all_x = np.float32(np.random.uniform(-2*math.pi, 2*math.pi,(1, NUM_points))).T\n",
    "np.random.shuffle(all_x)\n",
    "train_size = int(900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 900 points are in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training = all_x[:train_size]\n",
    "y_training = function_to_learn(x_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 100 will be in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = all_x[train_size:]\n",
    "y_validation = function_to_learn(x_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib, we display these sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.scatter(x_training, y_training, c='blue', label='train')\n",
    "plt.scatter(x_validation, y_validation,c='red',label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build the model</h2>\n",
    "First, we create the placeholders for the input tensor (X) and the output tensor (Y):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 1], name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we build the hidden layer of [1 x 10] dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = tf.Variable(tf.random_uniform([1, layer_1_neurons], minval=-1, maxval=1, dtype=tf.float32))\n",
    "b_h = tf.Variable(tf.zeros([1, layer_1_neurons], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It receives the input value from the X input tensor, combined with the weight w_hij connections and added with the respective biases of layer 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tf.nn.sigmoid(tf.matmul(X, w_h) + b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer is a [10 x 1] tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_o = tf.Variable(tf.random_uniform([layer_1_neurons, 1], minval=-1, maxval=1, dtype=tf.float32))\n",
    "b_o = tf.Variable(tf.zeros([1, 1], dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron in this second layer receives inputs from the neurons of layer 1, combined with weight w_oij connections and added together with the respective biases of the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.matmul(h, w_o) + b_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define our optimizer for the newly defined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer().minimize(tf.nn.l2_loss(model - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that in this case, the cost function adopted is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.l2_loss(model - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.nn.l2_loss function is a TensorFlow that computes half the L2 norm of a tensor without the sqrt, that is, the output for the preceding function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.sum((model - Y) ** 2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Launch the session</h2>\n",
    "Let's build the evaluation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can launch the learning session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    for start, end in zip(range(0, len(x_training), batch_size), range(batch_size, len(x_training), batch_size)):\n",
    "        sess.run(train_op, feed_dict={X: x_training[start:end], Y: y_training[start:end]})\n",
    "    cost = sess.run(tf.nn.l2_loss(model - y_validation), feed_dict={X:x_validation})\n",
    "    errors.append(cost)\n",
    "    if i%100 == 0: \n",
    "        print(\"epoch %d, cost = %g\" % (i, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code allow us to display how the cost changes in the running epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors,label='MLP Function Approximation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cost')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
